<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="Airbnb_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Airbnb_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Airbnb_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Airbnb_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Airbnb_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="Airbnb_files/navigation-1.1/tabsets.js"></script>
<link href="Airbnb_files/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="Airbnb_files/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">




</div>


<div id="airbnb-analysis" class="section level2">
<h2>Airbnb Analysis</h2>
<div id="summary" class="section level3">
<h3>Summary</h3>
</div>
<div id="data-processing-overview-and-sqlite-data-transfer" class="section level3">
<h3>Data Processing (Overview and SQLite Data Transfer)</h3>
<p>All data used in this analysis was downloaded from the <a href="http://insideairbnb.com/">Inside Airbnb</a> website. Data files are available in the <a href="http://insideairbnb.com/get-the-data.html">‘Get the Data’</a> section of the website. This analysis uses the listings (‘listings.csv.gz’), calendar (‘calendar.csv.gz’), and reviews (‘reviews.csv.gz’) files.</p>
<p>Data files were downloaded from the <a href="http://insideairbnb.com/">Inside Airbnb</a> website and placed in a local directory, with a child-folder structure of [Country]/[State]/[City]/[Data Scrape Date (‘YYYY-MM-DD’)]/[GZ File]. The directory structure used the city, state, and country names from the headers of each scraped city on the <a href="http://insideairbnb.com/get-the-data.html">‘Get the Data’</a> page of the <a href="http://insideairbnb.com/">Inside Airbnb</a> website. All files for all scrapes of all cities were downloaded as of April 2, 2017, barring the December 2, 2015 scrape of New York City (this scrape contained a broken link for the calendar file). This data encompassed 136 data scrapes for 43 distinct cities. Older scrapes for each city can be removed to cut down on the data size substantially.</p>
<p>This code uses the directory structure of the data folder to construct the metadata for the scraped data. It is therefore imperative that the directory structure is properly set-up. The listings, calendar, and reviews files must retain their original file names.</p>
<p>For this first step in the data processing, the data is placed into a local SQLite database. The remaining analysis can either be performed on this SQLite database, or the data can be transferred to another store (e.g. a MySQL server on an AWS RDS instance). Total disk space for the gz files (408 files total) is about 7.95 GBs. Total disk space for the complete SQLite database is about 70 GBs, including the optional indices.</p>
<pre class="r"><code>library(RSQLite)
library(DBI)</code></pre>
<pre class="r"><code># Directory for data folder
data_directory &lt;- &#39;C:/Airbnb&#39;

# Directory and file for SQLite database
sqlite_directory &lt;- &#39;C:/SQLite/Databases/Airbnb.sqlite3&#39;</code></pre>
<p>Connect to or create the SQLite database and build the tables if they do not exist</p>
<pre class="r"><code># Connect to or create SQLite database
abnb_db &lt;- dbConnect(SQLite(), sqlite_directory)</code></pre>
<pre class="r"><code># Create tables if they do not exist
calendar_create &lt;- &#39;CREATE TABLE IF NOT EXISTS Calendar (
                        Calendar_ID INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL UNIQUE,
                        DataScrape_ID INTEGER NOT NULL,
                        ListingID INTEGER NOT NULL,
                        Date DATE NOT NULL,
                        Available VARCHAR NOT NULL,
                        Price REAL NOT NULL
                    );
                    &#39;
dbExecute(abnb_db, calendar_create)
listings_create &lt;- &#39;CREATE TABLE IF NOT EXISTS Listings (
                        Listings_ID INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE NOT NULL,
                        DataScrape_ID INTEGER NOT NULL,
                        ID INTEGER NOT NULL,
                        ListingURL VARCHAR,
                        ScrapeID INTEGER,
                        LastSearched DATE,
                        LastScraped DATE,
                        Name TEXT,
                        Summary TEXT,
                        Space TEXT,
                        Description TEXT,
                        ExperiencesOffered TEXT,
                        NeighborhoodOverview TEXT,
                        Notes TEXT,
                        Transit TEXT,
                        Access TEXT,
                        Interaction TEXT,
                        HouseRules TEXT,
                        ThumbnailURL VARCHAR,
                        MediumURL VARCHAR,
                        PictureURL VARCHAR,
                        XLPictureURL VARCHAR,
                        HostID INTEGER,
                        HostURL VARCHAR,
                        HostName VARCHAR,
                        HostSince DATE,
                        HostLocation VARCHAR,
                        HostAbout TEXT,
                        HostResponseTime VARCHAR,
                        HostResponseRate REAL,
                        HostAcceptanceRate REAL,
                        HostIsSuperhost VARCHAR,
                        HostThumbnailURL VARCHAR,
                        HostPictureURL VARCHAR,
                        HostNeighborhood VARCHAR,
                        HostListingsCount INTEGER,
                        HostTotalListingsCount INTEGER,
                        HostVerifications VARCHAR,
                        HostHasProfilePic VARCHAR,
                        HostIdentityVerified VARCHAR,
                        Street VARCHAR,
                        Neighborhood VARCHAR,
                        NeighborhoodCleansed VARCHAR,
                        NeighborhoodGroupCleansed VARCHAR,
                        City VARCHAR,
                        State VARCHAR,
                        ZipCode VARCHAR,
                        Market VARCHAR,
                        SmartLocation VARCHAR,
                        CountryCode VARCHAR,
                        Country VARCHAR,
                        Latitude VARCHAR,
                        Longitude VARCHAR,
                        IsLocationExact VARCHAR,
                        PropertyType VARCHAR,
                        RoomType VARCHAR,
                        Accommodates INTEGER,
                        Bathrooms INTEGER,
                        Bedrooms INTEGER,
                        Beds INTEGER,
                        BedType VARCHAR,
                        Amenities VARCHAR,
                        SquareFeet REAL,
                        Price REAL,
                        WeeklyPrice REAL,
                        MonthlyPrice REAL,
                        SecurityDeposit REAL,
                        CleaningFee REAL,
                        GuestsIncluded INTEGER,
                        ExtraPeople REAL,
                        MinimumNights INTEGER,
                        MaximumNights INTEGER,
                        CalendarUpdated VARCHAR,
                        HasAvailability VARCHAR,
                        Availability30 INTEGER,
                        Availability60 INTEGER,
                        Availability90 INTEGER,
                        Availability365 INTEGER,
                        CalendarLastScraped DATE,
                        NumberOfReviews INTEGER,
                        FirstReview DATE,
                        LastReview DATE,
                        ReviewScoresRating INTEGER,
                        ReviewScoresAccuracy INTEGER,
                        ReviewScoresCleanliness INTEGER,
                        ReviewScoresCheckIn INTEGER,
                        ReviewScoresCommunication INTEGER,
                        ReviewScoresLocation INTEGER,
                        ReviewScoresValue INTEGER,
                        RequiresLicense VARCHAR,
                        License VARCHAR,
                        JurisdictionNames VARCHAR,
                        InstantBookable VARCHAR,
                        CancellationPolicy VARCHAR,
                        RequireGuestProfilePicture VARCHAR,
                        RequireGuestPhoneVerification VARCHAR,
                        RegionID INTEGER,
                        RegionName VARCHAR,
                        RegionParentID INTEGER,
                        CalculatedHostListingsCount INTEGER,
                        ReviewsPerMonth REAL
                    );
                    &#39;
dbExecute(abnb_db, listings_create)
reviews_create &lt;- &#39;CREATE TABLE IF NOT EXISTS Reviews (
                       Reviews_ID INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE NOT NULL,
                       DataScrape_ID INTEGER NOT NULL,
                       ListingID INTEGER NOT NULL,
                       ID INTEGER NOT NULL,
                       Date DATE NOT NULL,
                       ReviewerID INTEGER NOT NULL,
                       ReviewerName VARCHAR NOT NULL,
                       Comments TEXT NOT NULL
                   );
                   &#39;
dbExecute(abnb_db, reviews_create)
map_data_create &lt;- &#39;CREATE TABLE IF NOT EXISTS Map_DataScrape (
                        DataScrape_ID INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE NOT NULL,
                        Country VARCHAR NOT NULL,
                        State VARCHAR NOT NULL,
                        City VARCHAR NOT NULL,
                        DataScrapeDate DATE NOT NULL
                    );
                    &#39;
dbExecute(abnb_db, map_data_create)</code></pre>
<p>Add unique index to mapping table, to prevent duplication of scrapes in event of multiple runs</p>
<pre class="r"><code>idx_datascrape_unique &lt;- &#39;CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_scrape ON Map_DataScrape (
                              Country
                              ,State
                              ,City
                              ,DataScrapeDate
                          );
                          &#39;
dbExecute(abnb_db, idx_datascrape_unique)</code></pre>
<p>Investigate directory structure of data folder, and create scrape metadata from folder names</p>
<pre class="r"><code>dir_vec &lt;- list.files(data_directory, recursive = TRUE)
dir_df &lt;- t(data.frame(strsplit(dir_vec, split = &#39;/&#39;), stringsAsFactors = FALSE))
dir_df &lt;- unique(dir_df[, 1:4])
colnames(dir_df) &lt;- c(&#39;Country&#39;, &#39;State&#39;, &#39;City&#39;, &#39;DataScrapeDate&#39;)
rownames(dir_df) &lt;- seq.int(1, nrow(dir_df))
head(dir_df, 10)</code></pre>
<pre><code>  Country         State        City        DataScrapeDate
1 &quot;United States&quot; &quot;California&quot; &quot;San Diego&quot; &quot;2015-06-22&quot;  
2 &quot;United States&quot; &quot;California&quot; &quot;San Diego&quot; &quot;2016-07-07&quot;  </code></pre>
<pre class="r"><code># Find max data scrape mapping ID already in the SQLite database
max_scrape &lt;- dbExecute(abnb_db, &#39;SELECT MAX(DataScrape_ID) FROM Map_DataScrape;&#39;)</code></pre>
<p>Define column name data frame for listings table (listings files differ from one city and scrape to another – this step and several steps within the read code for the listings data must be completed in order to maintain consistency across different data files)</p>
<pre class="r"><code># Possible column names for listings data from gz source files
listings_ds_colnames &lt;- c(&#39;Listings_ID&#39;, &#39;DataScrape_ID&#39;, &#39;id&#39;, &#39;listing_url&#39;, &#39;scrape_id&#39;, &#39;last_searched&#39;,
                          &#39;last_scraped&#39;, &#39;name&#39;, &#39;summary&#39;, &#39;space&#39;, &#39;description&#39;, &#39;experiences_offered&#39;,
                          &#39;neighborhood_overview&#39;, &#39;notes&#39;, &#39;transit&#39;, &#39;access&#39;, &#39;interaction&#39;, &#39;house_rules&#39;,
                          &#39;thumbnail_url&#39;, &#39;medium_url&#39;, &#39;picture_url&#39;, &#39;xl_picture_url&#39;, &#39;host_id&#39;,
                          &#39;host_url&#39;, &#39;host_name&#39;, &#39;host_since&#39;, &#39;host_location&#39;, &#39;host_about&#39;,
                          &#39;host_response_time&#39;, &#39;host_response_rate&#39;, &#39;host_acceptance_rate&#39;,
                          &#39;host_is_superhost&#39;, &#39;host_thumbnail_url&#39;, &#39;host_picture_url&#39;, &#39;host_neighbourhood&#39;,
                          &#39;host_listings_count&#39;, &#39;host_total_listings_count&#39;, &#39;host_verifications&#39;,
                          &#39;host_has_profile_pic&#39;, &#39;host_identity_verified&#39;, &#39;street&#39;, &#39;neighbourhood&#39;,
                          &#39;neighbourhood_cleansed&#39;, &#39;neighbourhood_group_cleansed&#39;, &#39;city&#39;, &#39;state&#39;,
                          &#39;zipcode&#39;, &#39;market&#39;, &#39;smart_location&#39;, &#39;country_code&#39;, &#39;country&#39;, &#39;latitude&#39;,
                          &#39;longitude&#39;, &#39;is_location_exact&#39;, &#39;property_type&#39;, &#39;room_type&#39;, &#39;accommodates&#39;,
                          &#39;bathrooms&#39;, &#39;bedrooms&#39;, &#39;beds&#39;, &#39;bed_type&#39;, &#39;amenities&#39;, &#39;square_feet&#39;, &#39;price&#39;,
                          &#39;weekly_price&#39;, &#39;monthly_price&#39;, &#39;security_deposit&#39;, &#39;cleaning_fee&#39;,
                          &#39;guests_included&#39;, &#39;extra_people&#39;, &#39;minimum_nights&#39;, &#39;maximum_nights&#39;,
                          &#39;calendar_updated&#39;, &#39;has_availability&#39;, &#39;availability_30&#39;, &#39;availability_60&#39;,
                          &#39;availability_90&#39;, &#39;availability_365&#39;, &#39;calendar_last_scraped&#39;, &#39;number_of_reviews&#39;,
                          &#39;first_review&#39;, &#39;last_review&#39;, &#39;review_scores_rating&#39;, &#39;review_scores_accuracy&#39;,
                          &#39;review_scores_cleanliness&#39;, &#39;review_scores_checkin&#39;, &#39;review_scores_communication&#39;,
                          &#39;review_scores_location&#39;, &#39;review_scores_value&#39;, &#39;requires_license&#39;, &#39;license&#39;,
                          &#39;jurisdiction_names&#39;, &#39;instant_bookable&#39;, &#39;cancellation_policy&#39;,
                          &#39;require_guest_profile_picture&#39;, &#39;require_guest_phone_verification&#39;, &#39;region_id&#39;,
                          &#39;region_name&#39;, &#39;region_parent_id&#39;, &#39;calculated_host_listings_count&#39;,
                          &#39;reviews_per_month&#39;)

# Column names for listings data from the SQLite database
listings_db_colnames &lt;- c(&#39;Listings_ID&#39;, &#39;DataScrape_ID&#39;, &#39;ID&#39;, &#39;ListingURL&#39;, &#39;ScrapeID&#39;, &#39;LastSearched&#39;,
                          &#39;LastScraped&#39;, &#39;Name&#39;, &#39;Summary&#39;, &#39;Space&#39;, &#39;Description&#39;, &#39;ExperiencesOffered&#39;,
                          &#39;NeighborhoodOverview&#39;, &#39;Notes&#39;, &#39;Transit&#39;, &#39;Access&#39;, &#39;Interaction&#39;, &#39;HouseRules&#39;,
                          &#39;ThumbnailURL&#39;, &#39;MediumURL&#39;, &#39;PictureURL&#39;, &#39;XLPictureURL&#39;, &#39;HostID&#39;,
                          &#39;HostURL&#39;, &#39;HostName&#39;, &#39;HostSince&#39;, &#39;HostLocation&#39;, &#39;HostAbout&#39;,
                          &#39;HostResponseTime&#39;, &#39;HostResponseRate&#39;, &#39;HostAcceptanceRate&#39;,
                          &#39;HostIsSuperhost&#39;, &#39;HostThumbnailURL&#39;, &#39;HostPictureURL&#39;, &#39;HostNeighborhood&#39;,
                          &#39;HostListingsCount&#39;, &#39;HostTotalListingsCount&#39;, &#39;HostVerifications&#39;,
                          &#39;HostHasProfilePic&#39;, &#39;HostIdentityVerified&#39;, &#39;Street&#39;, &#39;Neighborhood&#39;,
                          &#39;NeighborhoodCleansed&#39;, &#39;NeighborhoodGroupCleansed&#39;, &#39;City&#39;, &#39;State&#39;,
                          &#39;ZipCode&#39;, &#39;Market&#39;, &#39;SmartLocation&#39;, &#39;CountryCode&#39;, &#39;Country&#39;, &#39;Latitude&#39;,
                          &#39;Longitude&#39;, &#39;IsLocationExact&#39;, &#39;PropertyType&#39;, &#39;RoomType&#39;, &#39;Accommodates&#39;,
                          &#39;Bathrooms&#39;, &#39;Bedrooms&#39;, &#39;Beds&#39;, &#39;BedType&#39;, &#39;Amenities&#39;, &#39;SquareFeet&#39;, &#39;Price&#39;,
                          &#39;WeeklyPrice&#39;, &#39;MonthlyPrice&#39;, &#39;SecurityDeposit&#39;, &#39;CleaningFee&#39;,
                          &#39;GuestsIncluded&#39;, &#39;ExtraPeople&#39;, &#39;MinimumNights&#39;, &#39;MaximumNights&#39;,
                          &#39;CalendarUpdated&#39;, &#39;HasAvailability&#39;, &#39;Availability30&#39;, &#39;Availability60&#39;,
                          &#39;Availability90&#39;, &#39;Availability365&#39;, &#39;CalendarLastScraped&#39;, &#39;NumberOfReviews&#39;,
                          &#39;FirstReview&#39;, &#39;LastReview&#39;, &#39;ReviewScoresRating&#39;, &#39;ReviewScoresAccuracy&#39;,
                          &#39;ReviewScoresCleanliness&#39;, &#39;ReviewScoresCheckIn&#39;, &#39;ReviewScoresCommunication&#39;,
                          &#39;ReviewScoresLocation&#39;, &#39;ReviewScoresValue&#39;, &#39;RequiresLicense&#39;, &#39;License&#39;,
                          &#39;JurisdictionNames&#39;, &#39;InstantBookable&#39;, &#39;CancellationPolicy&#39;,
                          &#39;RequireGuestProfilePicture&#39;, &#39;RequireGuestPhoneVerification&#39;, &#39;RegionID&#39;,
                          &#39;RegionName&#39;, &#39;RegionParentID&#39;, &#39;CalculatedHostListingsCount&#39;,
                          &#39;ReviewsPerMonth&#39;)

# Combine gz file data source and SQLite column names for listings data into mapping data frame
listings_df_colnames &lt;- data.frame(listings_ds_colnames, listings_db_colnames)</code></pre>
<p>Loop through all directories (as defined in dir_df matrix), read all gz files, and populated data into the SQLite database</p>
<pre class="r"><code># Read calendar data and populate into SQLite database
loop &lt;- 1 + max_scrape
while (loop &lt;= (max_scrape + nrow(dir_df))) {
    # Repair columns if broken
    calendar_temp &lt;- readLines(paste(data_directory, &#39;/&#39;, dir_df[loop, 1], &#39;/&#39;,
                                     dir_df[loop, 2], &#39;/&#39;, dir_df[loop, 3], &#39;/&#39;,
                                     dir_df[loop, 4], &#39;/calendar.csv.gz&#39;, sep = &#39;&#39;))
    # Replace two consecutive double quotes with one double quote and wrap currency figures in a double quote
    calendar_temp &lt;- gsub(&#39;&quot;&quot;&#39;, &#39;&quot;&#39;, gsub(&#39;([$][0-9,.]+)&#39;, &#39;&quot;\\1&quot;&#39;, calendar_temp))
    # Write temp data to temporary file
    write(calendar_temp, file = &#39;temp.txt&#39;, ncolumns = 1)
    # Prepare calendar data frame
    calendar &lt;- read.table(&#39;temp.txt&#39;, sep = &#39;,&#39;, quote = &#39;&quot;&#39;, header = TRUE, comment.char = &#39;&#39;,
                           stringsAsFactors = FALSE)
    # Remove NAs (interpreted as NULL)
    calendar[is.na(calendar)] &lt;- &#39;&#39;
    # Create extra data rows
    cal_length &lt;- nrow(calendar)
    id_vec_cal &lt;- rep(NA, cal_length)
    map_id_vec_cal &lt;- rep(loop, cal_length)
    # Bind data frame
    calendar &lt;- cbind.data.frame(id_vec_cal, map_id_vec_cal, calendar)
    # Set column names
    names(calendar) &lt;- c(&#39;Calendar_ID&#39;, &#39;DataScrape_ID&#39;, &#39;ListingID&#39;, &#39;Date&#39;, &#39;Available&#39;, &#39;Price&#39;)
    # Strip dollar signs and commas from prices
    calendar$Price &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, calendar$Price))
    # Write calendar data frame to SQLite database
    dbWriteTable(abnb_db, name = &#39;Calendar&#39;, value = calendar, append = TRUE)
    
    # Increment loop
    loop &lt;- loop + 1
}

# Remove temp file and large objects
if (file.exists(&#39;temp.txt&#39;)) {file.remove(&#39;temp.txt&#39;)}
if (exists(&#39;calendar_temp&#39;)) {rm(calendar_temp)}
if (exists(&#39;calendar&#39;)) {rm(calendar)}</code></pre>
<pre class="r"><code># Read listings data and populate into SQLite database
loop &lt;- 1 + max_scrape
while (loop &lt;= (max_scrape + nrow(dir_df))) {
    # Repair columns if broken
    listings_temp &lt;- readLines(paste(data_directory, &#39;/&#39;, dir_df[loop, 1], &#39;/&#39;,
                                     dir_df[loop, 2], &#39;/&#39;, dir_df[loop, 3], &#39;/&#39;,
                                     dir_df[loop, 4], &#39;/listings.csv.gz&#39;, sep = &#39;&#39;))
    # Fix quoting characters and replace two consecutive double quotes with a single quote
    listings_temp &lt;- gsub(&#39;,\&#39;&quot;&#39;, &#39;,&quot;\&#39;&#39;, gsub(&#39;&quot;&quot;&#39;, &#39;\&#39;&#39;, listings_temp))
    # Write temp data to temporary file
    write(listings_temp, file = &#39;temp.txt&#39;, ncolumns = 1)
    # Prepare listings data frame
    listings &lt;- read.table(&#39;temp.txt&#39;, sep = &#39;,&#39;, quote = &#39;&quot;&#39;, header = TRUE, comment.char = &#39;&#39;,
                           stringsAsFactors = FALSE)
    # Remove NAs (interpreted as NULL)
    listings[is.na(listings)] &lt;- &#39;&#39;
    # Create extra data rows
    lst_length &lt;- nrow(listings)
    id_vec_lst &lt;- rep(NA, lst_length)
    map_id_vec_lst &lt;- rep(loop, lst_length)
    # Add extra data rows to data frame
    listings[&#39;Listings_ID&#39;] &lt;- id_vec_lst
    listings[&#39;DataScrape_ID&#39;] &lt;- map_id_vec_lst
    # Set column names
    i &lt;- 1
    while (i &lt;= length(names(listings))) {
        names(listings)[i] &lt;- as.character(listings_df_colnames$listings_db_colnames
                                           [match(names(listings)[i],
                                                  listings_df_colnames$listings_ds_colnames)])
        i &lt;- i + 1
    }
    # Fill in missing columns with NA
    i &lt;- 1
    while (i &lt;= length(listings_df_colnames$listings_db_colnames)) {
        if (!is.element(listings_df_colnames$listings_db_colnames[i], names(listings))) {
            temp_vec &lt;- rep(NA, lst_length)
            listings[as.character(listings_df_colnames$listings_db_colnames[i])] &lt;- temp_vec
        }
        i &lt;- i + 1
    }
    # Reorder data frame
    i &lt;- 1
    name_vec &lt;- vector(mode = &#39;integer&#39;)
    while (i &lt;= length(listings_df_colnames$listings_db_colnames)) {
        name_vec[i] &lt;- match(listings_df_colnames$listings_db_colnames[i], names(listings))
        i &lt;- i + 1
    }
    listings &lt;- listings[name_vec]
    # Strip dollar signs and commas from prices
    listings$Price &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$Price))
    listings$WeeklyPrice &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$WeeklyPrice))
    listings$MonthlyPrice &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$MonthlyPrice))
    listings$SecurityDeposit &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$SecurityDeposit))
    listings$CleaningFee &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$CleaningFee))
    listings$ExtraPeople &lt;- gsub(&#39;,&#39;, &#39;&#39;, gsub(&#39;\\$&#39;, &#39;&#39;, listings$ExtraPeople))
    # Strip percent signs
    listings$HostResponseRate &lt;- gsub(&#39;%&#39;, &#39;&#39;, listings$HostResponseRate)
    listings$HostAcceptanceRate &lt;- gsub(&#39;%&#39;, &#39;&#39;, listings$HostAcceptanceRate)
    # Write listings data frame to SQLite database
    dbWriteTable(abnb_db, name = &#39;Listings&#39;, value = listings, append = TRUE)
    
    # Increment loop
    loop &lt;- loop + 1
}

# Remove temp file and large objects
if (file.exists(&#39;temp.txt&#39;)) {file.remove(&#39;temp.txt&#39;)}
if (exists(&#39;listings_temp&#39;)) {rm(listings_temp)}
if (exists(&#39;listings&#39;)) {rm(listings)}</code></pre>
<pre class="r"><code># Read reviews data and populate into SQLite database
loop &lt;- 1 + max_scrape
while (loop &lt;= (max_scrape + nrow(dir_df))) {
    # Prepare reviews data frame
    reviews &lt;- read.table(paste(data_directory, &#39;/&#39;, dir_df[loop, 1], &#39;/&#39;,
                                dir_df[loop, 2], &#39;/&#39;, dir_df[loop, 3], &#39;/&#39;,
                                dir_df[loop, 4], &#39;/reviews.csv.gz&#39;, sep = &#39;&#39;),
                          sep = &#39;,&#39;, quote = &#39;&quot;&#39;, header = TRUE, comment.char = &#39;&#39;,
                          stringsAsFactors = FALSE)
    reviews[is.na(reviews)] &lt;- &#39;&#39;
    # Create extra data rows
    rvw_length &lt;- nrow(reviews)
    id_vec_rvw &lt;- rep(NA, rvw_length)
    map_id_vec_rvw &lt;- rep(loop, rvw_length)
    # Bind data frame
    reviews &lt;- cbind.data.frame(id_vec_rvw, map_id_vec_rvw, reviews)
    # Set column names
    names(reviews) &lt;- c(&#39;Reviews_ID&#39;, &#39;DataScrape_ID&#39;, &#39;ListingID&#39;, &#39;ID&#39;, &#39;Date&#39;, &#39;ReviewerID&#39;,
                        &#39;ReviewerName&#39;, &#39;Comments&#39;)
    # Write reviews data frame to SQLite database
    dbWriteTable(abnb_db, name = &#39;Reviews&#39;, value = reviews, append = TRUE)
    
    # Increment loop
    loop &lt;- loop + 1
}

# Remove large objects
if (exists(&#39;reviews&#39;)) {rm(reviews)}</code></pre>
<pre class="r"><code># Create data scrape mapping table
scrape_seq &lt;- seq(1 + max_scrape, (max_scrape + nrow(dir_df)))
data_scrape &lt;- cbind.data.frame(scrape_seq, dir_df)
# Set column names
colnames(data_scrape) &lt;- c(&#39;DataScrape_ID&#39;, &#39;Country&#39;, &#39;State&#39;, &#39;City&#39;, &#39;DataScrapeDate&#39;)
# Write data scrape mapping data frame to SQLite database
dbWriteTable(abnb_db, name = &#39;Map_DataScrape&#39;, value = data_scrape, append = TRUE, row.names = FALSE)</code></pre>
<p><strong>Optional:</strong> Create database indices for faster querying</p>
<pre class="r"><code># Data scrape ID indices
idx_datascrape_id &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_DataScrape_ID ON Map_DataScrape (
                          DataScrape_ID
                      );
                      &#39;
dbExecute(abnb_db, idx_datascrape_id)
idx_datascrape_id_cal &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_DataScrape_ID_cal ON Calendar (
                              DataScrape_ID
                          );
                          &#39;
dbExecute(abnb_db, idx_datascrape_id_cal)
idx_datascrape_id_lst &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_DataScrape_ID_lst ON Listings (
                              DataScrape_ID
                          );
                          &#39;
dbExecute(abnb_db, idx_datascrape_id_lst)
idx_datascrape_id_rvw &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_DataScrape_ID_rvw ON Reviews (
                              DataScrape_ID
                          );
                          &#39;
dbExecute(abnb_db, idx_datascrape_id_rvw)

# Listing ID indices
idx_listing_id_cal &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_Listing_ID_cal ON Calendar (
                           ListingID
                       );
                       &#39;
dbExecute(abnb_db, idx_listing_id_cal)
idx_listing_id_lst &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_Listing_ID_lst ON Listings (
                           ID
                       );
                       &#39;
dbExecute(abnb_db, idx_listing_id_lst)
idx_listing_id_rvw &lt;- &#39;CREATE INDEX IF NOT EXISTS idx_Listing_ID_rvw ON Reviews (
                           ListingID
                       );
                       &#39;
dbExecute(abnb_db, idx_listing_id_rvw)</code></pre>
<p>Disconnect from the SQLite database</p>
<pre class="r"><code># Disconnect from SQLite database
dbDisconnect(abnb_db)</code></pre>
</div>
<div id="data-processing-aws-mysql-rds-instance-data-transfer" class="section level3">
<h3>Data Processing (AWS MySQL RDS Instance Data Transfer)</h3>
<p>Although merging all bz files into a SQLite database with the proper metadata enables significantly streamlined analysis, this approach towards data management carries significant downsides. The data takes up a large amount of space and is stored twice on disk (once in the assortment of gz files and once in the SQLite database). The analysis is limited by the processing power and memory of the user’s computer. Finally, the SQLite database is serverless and therefore must be transferred to another user’s computer in order to facilitate data sharing.</p>
<p>Transferring the data to a RDBMS solves these problems. Below, the data from the SQLite database will be moved to a MySQL server on an AWS RDS instance owned by the analysis author. There is no need for the SQLite database to server as an intermediary, however. In this analysis, the data will be moved out of the SQLite database and into the MySQL server simply to reduce duplication of code. Below are documented the steps required to alter the code in the ‘Data Processing’ sections of this analysis to move the data straight from the bz files into the MySQL server.</p>
<ol style="list-style-type: decimal">
<li>Replace all table and index creation statements with the equivalent MySQL statements documented below.</li>
<li>Replace the statement used to create the max_scrape variable with the equivalent MySQL statement documented below.</li>
<li>Perform the following modification to the read and populate blocks for the calendar, listings, reviews, and data scrape code blocks:
<ul>
<li>Remove all calls to dbWriteTable.</li>
<li>In place of the dbWriteTable calls, write the final dataframes to a text file. Make sure to replace all new line, carriage return, and tab characters in fields where these can occur (see statements below to identify these).</li>
<li>Execute LOAD DATA LOCAL INFILE statements on the MySQL server using these files (see statements below to see examples).</li>
<li>Additional work may need to be done to handle differences in how MySQL handles NULL fields (concerning how R writes NA values and how MySQL reads the resulting data, and how MySQL handles NULL values for auto-increment NOT NULL columns).</li>
</ul></li>
</ol>
</div>
<div id="analysis" class="section level3">
<h3>Analysis</h3>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
